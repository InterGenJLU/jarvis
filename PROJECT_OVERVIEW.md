# JARVIS - Personal AI Assistant

**Version:** 2.10.0 (Production Ready)
**Last Updated:** February 26, 2026
**Status:** ‚úÖ Stable, Feature-Rich, Voice-Controlled

---

## üìã Table of Contents
- [What is JARVIS?](#what-is-jarvis)
- [Current Capabilities](#current-capabilities)
- [System Architecture](#system-architecture)
- [Technology Stack](#technology-stack)
- [Progress Timeline](#progress-timeline)
- [Design Philosophy](#design-philosophy)
- [Roadmap](#roadmap)
- [Getting Started](#getting-started)

---

## ü§ñ What is JARVIS?

JARVIS (Just A Rather Very Intelligent System) is a fully offline, voice-controlled AI assistant. Unlike commercial assistants, JARVIS runs entirely on your local hardware with:

- ‚úÖ **Complete Privacy** - No cloud, no data collection
- ‚úÖ **Custom Voice Training** - Learns YOUR accent
- ‚úÖ **Modular Skills** - Easy to extend
- ‚úÖ **Natural Conversation** - Semantic understanding
- ‚úÖ **Production Ready** - Stable, tested, reliable

**Hardware:** Runs on consumer-grade PC (Ryzen 9 5900X, AMD RX 7900 XT)  
**OS:** Ubuntu 24.04 LTS  
**Latency:** 300-600ms for skill queries, 2-4s for LLM fallback (streaming)

---

## üéØ Current Capabilities

### Core Features
- **Wake Word Detection** - Porcupine "Jarvis" with 100% accuracy
- **Speech Recognition** - Fine-tuned Whisper v2 (CTranslate2, GPU-accelerated, 94%+ accuracy, 198 phrases, Southern accent)
- **Natural Language Understanding** - Semantic intent matching (sentence-transformers)
- **Conversational Flow Engine** - Persona module (24 response pools, ~90 templates), ConversationState (turn tracking), ConversationRouter (shared priority chain)
- **Text-to-Speech** - Kokoro 82M (primary, CPU, fable+george blend) + Piper ONNX fallback
- **LLM Intelligence** - Qwen3.5-35B-A3B (Q3_K_M, MoE, 3B active params) via llama.cpp + Claude API fallback with quality gating
- **LLM-Centric Tool Calling (Phases 1-2)** - Skills migrated to native Qwen3.5 tool calling. 7 tools (6 domain + web_search). Semantic pruning selects tools; LLM decides. Time/date handled by TimeInfoSkill (instant response). 100% accuracy across 1,200+ trials
- **Web Research** - Qwen3.5 native tool calling + DuckDuckGo + trafilatura, multi-source synthesis
- **Event-Driven Pipeline** - Coordinator with STT/TTS workers, streaming LLM, contextual ack cache (10 tagged phrases)
- **Gapless TTS Streaming** - StreamingAudioPipeline with single persistent aplay, background Kokoro generation
- **Adaptive Conversation Windows** - 4-7s duration, extends with conversation depth, timeout cleanup, noise filtering, dismissal detection
- **Ambient Wake Word Filter** - Multi-signal: position, copula, threshold 0.80, length ‚Äî blocks ambient mentions
- **Self-Awareness** - Capability manifest + system state injected into LLM context ‚Äî JARVIS knows what it can do, its current state, and skill reliability
- **Task Planner** - Compound request detection (22 signals), LLM plan generation, sequential execution with per-step LLM evaluation, pause/resume/cancel/skip voice interrupts, predictive timing announcements, error-aware + context-budget-aware planning
- **LLM Metrics Dashboard** - Real-time tracking (latency, tokens, errors), web dashboard at `/metrics`, persistent SQLite, per-skill breakdowns
- **People Manager + Social Introductions** - "Meet my niece Arya" triggers multi-turn intro flow (name confirmation, pronunciation check, fact gathering). SQLite people database with TTS pronunciation overrides and LLM context injection for known contacts
- **Three Frontends** - Voice (production), console (debug/hybrid), web UI (browser-based chat with streaming + sessions)
- **Web UI** - aiohttp WebSocket server, streaming LLM, markdown rendering, session sidebar, health HUD, file handling

### Skills (12 Active)

#### üå§Ô∏è Weather
- Current conditions, forecasts, rain probability
- *"Jarvis, what's the weather like?"*

#### ‚è∞ Time & Date
- Current time, date, day of week
- *"Jarvis, what time is it?"*

#### üíª System Information
- CPU, memory, disk, uptime, network
- *"Jarvis, what's my CPU usage?"*

#### üóÇÔ∏è Filesystem
- File search, code line counting, script analysis
- *"Jarvis, how many lines of code in your codebase?"*

#### üìù File Editor
- Write, edit, read, delete files + list share contents
- LLM-generated content, confirmation flow for destructive operations
- *"Jarvis, write a backup script"*
- *"Jarvis, delete temp.txt"*

#### üõ†Ô∏è Developer Tools
- 13 intents: codebase search, git multi-repo, system admin, general shell
- "Show me" visual output, 3-tier safety (allowlist ‚Üí confirmation ‚Üí blocked)
- *"Jarvis, show me the git status"*

#### üåê Web Navigation
- Playwright-based search, result selection, page navigation
- Scroll pagination (YouTube/Reddit), window management
- *"Jarvis, search for Python async tutorials"*

#### üì∞ News
- 16 RSS feeds, urgency classification, semantic dedup
- Voice headline delivery, category filtering
- *"Jarvis, read me the tech headlines"*

#### üîî Reminders
- Priority tones, nag behavior, acknowledgment tracking
- Google Calendar two-way sync, dedicated JARVIS calendar
- Daily & weekly rundowns (state machine: offered ‚Üí re-asked ‚Üí deferred ‚Üí retry)
- *"Jarvis, remind me to call the dentist at 3pm"*

#### üñ•Ô∏è Desktop Control (App Launcher)
- 16 intents: launch/close apps, fullscreen/minimize/maximize, volume up/down/mute, workspace switch/move, focus app, list windows, clipboard read/write
- GNOME Shell extension D-Bus bridge for Wayland-native window management
- *"Jarvis, open Chrome"*
- *"Jarvis, volume up"*
- *"Jarvis, switch to workspace 2"*

#### üí¨ Conversation
- Greetings, small talk, acknowledgments, butler personality
- *"Jarvis, how are you?"*

#### ü§ù Social Introductions
- Multi-turn butler-style introduction flow with name confirmation and pronunciation checks
- People database: relationship tracking, fact storage, TTS pronunciation overrides
- *"Jarvis, meet my niece Arya"*
- *"Jarvis, who is Arya?"*
- *"Jarvis, forget Arya"*

### Additional Systems
- **Conversational Memory** - SQLite fact store + FAISS semantic search, recall, batch LLM extraction, proactive surfacing, forget/transparency
- **Context Window** - Topic-segmented working memory, relevance-scored assembly, cross-session persistence
- **User Profiles** - Speaker identification (resemblyzer d-vectors), dynamic honorifics, voice enrollment
- **Google Calendar** - OAuth, event CRUD, incremental sync, background polling
- **Cross-Session Memory** - Last 32 messages loaded from persistent history
- **Health Check** - 5-layer system diagnostic (ANSI terminal report + voice summary)
- **Hardware Failure Handling** - Startup retry, device monitoring, degraded mode, graceful recovery
- **GNOME Desktop Bridge** - Custom GNOME Shell extension (D-Bus), Wayland-native window management, wmctrl fallback
- **GitHub Publishing** - Automated redaction pipeline, PII verification, non-interactive `--auto` publish
- **Automated Test Suite** - 270 tests (112 unit + 130 routing + 28 LLM) across 9 phases

---

## üèóÔ∏è System Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       USER VOICE                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AUDIO INPUT (FIFINE K669B USB Mic - Mono 48kHz)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  VAD (Voice Activity Detection) - WebRTC VAD             ‚îÇ
‚îÇ  ‚Ä¢ Detects speech vs silence                            ‚îÇ
‚îÇ  ‚Ä¢ Triggers transcription                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STT (Speech-to-Text) - Custom Whisper Model v2          ‚îÇ
‚îÇ  ‚Ä¢ Fine-tuned on user's Southern accent                 ‚îÇ
‚îÇ  ‚Ä¢ 198 training phrases, 94%+ accuracy                  ‚îÇ
‚îÇ  ‚Ä¢ GPU-accelerated: 0.1-0.2s transcription              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  WAKE WORD DETECTION + AMBIENT FILTER                   ‚îÇ
‚îÇ  ‚Ä¢ Checks for "Jarvis" in transcript                    ‚îÇ
‚îÇ  ‚Ä¢ Fuzzy matching (threshold: 0.80)                     ‚îÇ
‚îÇ  ‚Ä¢ Ambient filter: position, copula, length             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CONVERSATION ROUTER - Priority Chain                   ‚îÇ
‚îÇ  Layer 1: Confirmation interception                     ‚îÇ
‚îÇ  Layer 2: Dismissal / conversation close                ‚îÇ
‚îÇ  Layer 2.6: Social introductions (multi-turn)           ‚îÇ
‚îÇ  Layer 3: Memory / context / news pull-up               ‚îÇ
‚îÇ  Pre-P4: Task planner (compound request detection)      ‚îÇ
‚îÇ  Layer 4: Exact match (time, date)                      ‚îÇ
‚îÇ  Layer 5: Keyword + semantic verify                     ‚îÇ
‚îÇ  Layer 6: Pure semantic matching                        ‚îÇ
‚îÇ  Layer 7: LLM fallback (Qwen3.5-35B-A3B)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SKILL EXECUTION                                         ‚îÇ
‚îÇ  ‚Ä¢ Modular skill system                                 ‚îÇ
‚îÇ  ‚Ä¢ Semantic intent handlers                             ‚îÇ
‚îÇ  ‚Ä¢ Error handling & logging                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM - Qwen3.5-35B-A3B via REST API + Claude fallback   ‚îÇ
‚îÇ  ‚Ä¢ MoE: 256 experts, 8+1 active (~3B active params)    ‚îÇ
‚îÇ  ‚Ä¢ Web research via native tool calling                 ‚îÇ
‚îÇ  ‚Ä¢ Conversational responses + technical reasoning       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TTS (Text-to-Speech) - Kokoro 82M + Piper fallback     ‚îÇ
‚îÇ  ‚Ä¢ 50/50 fable+george voice blend                       ‚îÇ
‚îÇ  ‚Ä¢ Natural intonation, streaming output                 ‚îÇ
‚îÇ  ‚Ä¢ CPU-only, low latency                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AUDIO OUTPUT                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow Example
```
User: "Jarvis, how many lines of code in your codebase?"
  ‚Üì (Audio captured)
VAD: Speech detected
  ‚Üì (Transcription triggered)
Whisper: "jarvis, how many lines of code in your codebase?"
  ‚Üì (Wake word check)
Wake Word: ‚úÖ Detected "jarvis" (similarity: 1.00)
  ‚Üì (Strip wake word)
Intent Matching: "how many lines of code in your codebase"
  ‚Üì (Semantic match)
Semantic Matcher: 0.95 score ‚Üí FilesystemSkill.count_code_lines
  ‚Üì (Execute handler)
Filesystem Skill: Count Python files, exclude venv
  ‚Üì (Return response)
Response: "My codebase contains 320,388 lines of Python code across 40 files, sir."
  ‚Üì (TTS)
Kokoro: Generates audio
  ‚Üì (Playback)
User: Hears response
```

---

## üõ†Ô∏è Technology Stack

### Core Components
| Component | Technology | Purpose |
|-----------|-----------|---------|
| **STT** | faster-whisper (CTranslate2, fine-tuned) | Speech recognition |
| **TTS** | Kokoro 82M (primary) + Piper (fallback) | Speech synthesis |
| **LLM** | Qwen3.5-35B-A3B (Q3_K_M via llama.cpp) + Claude API | Language understanding + web research |
| **VAD** | WebRTC VAD | Voice activity detection |
| **Wake Word** | Porcupine | Trigger detection |
| **Embeddings** | sentence-transformers | Intent matching |

### Infrastructure
- **OS:** Ubuntu 24.04 LTS
- **Python:** 3.12
- **Service Manager:** systemd (user services)
- **LLM Server:** llama-server (REST API)
- **Storage:**
  - Code: `~/jarvis/`
  - Skills: `/mnt/storage/jarvis/skills/`
  - Models: `/mnt/models/` (4TB dedicated drive)
  - Public repo: `~/jarvis-public/` ‚Üí `github.com/InterGenJLU/jarvis`

### Key Libraries
- `torch` (ROCm) - PyTorch for ML models (CPU-only for TTS)
- `ctranslate2` - GPU-accelerated Whisper inference
- `sentence-transformers` - Semantic intent matching
- `kokoro` - Primary TTS engine (82M model)
- `sounddevice` - Audio I/O
- `playwright` - Headless web navigation
- `faiss-cpu` - Vector search for conversational memory
- `numpy` - Array operations
- `requests` - HTTP client for LLM API
- `pyyaml` - Configuration

---

## üìà Progress Timeline

### Phase 1: Foundation (Days 1-3)
- ‚úÖ Basic voice loop (wake word ‚Üí command ‚Üí response)
- ‚úÖ Whisper integration (base model)
- ‚úÖ Piper TTS setup
- ‚úÖ Simple command patterns

### Phase 2: Skills System (Days 4-7)
- ‚úÖ Modular skill architecture
- ‚úÖ Weather skill (OpenWeatherMap API)
- ‚úÖ Time/date skill
- ‚úÖ System info skill
- ‚úÖ Conversation skill

### Phase 3: Intelligence (Days 8-10)
- ‚úÖ Semantic intent matching (90% pattern reduction)
- ‚úÖ LLM integration (Mistral 7B ‚Üí Qwen 2.5-7B ‚Üí Qwen 3-8B ‚Üí Qwen3-VL-8B ‚Üí Qwen3.5-35B-A3B)
- ‚úÖ Conversation context window
- ‚úÖ Intent confidence scoring

### Phase 4: Production Ready (Feb 9-10)
- ‚úÖ Git version control (3 repositories)
- ‚úÖ Automated backups (daily, systemd)
- ‚úÖ 4TB model storage setup
- ‚úÖ Paul Bettany voice cloning (proof-of-concept)
- ‚úÖ Comprehensive documentation

### Phase 5: Major Upgrades (Feb 11) üöÄ
- ‚úÖ **Qwen 3-8B LLM** (better reasoning)
- ‚úÖ **Custom Whisper training** (88%+ accuracy)
- ‚úÖ **Filesystem skill** (semantic file operations)
- ‚úÖ **Audio optimization** (no overflow)
- ‚úÖ **Skill development guide** (comprehensive docs)

### Phase 6: GPU + CTranslate2 (Feb 12-13) üöÄ
- ‚úÖ **GPU-Accelerated STT** ‚Äî CTranslate2 with ROCm on RX 7900 XT (0.1-0.2s)
- ‚úÖ **PyTorch + CTranslate2 coexistence** ‚Äî torch 2.10.0+rocm7.1 + CT2 4.7.1
- ‚úÖ **Three-repo architecture** ‚Äî code, skills, models on separate drives

### Phase 7: Feature Explosion (Feb 14-17) üöÄ
- ‚úÖ **12 critical bug fixes** ‚Äî Whisper pre-buffer, semantic routing, keyword greediness, VAD overlap, etc.
- ‚úÖ **News headlines** ‚Äî 16 RSS feeds, urgency classification, semantic dedup, voice delivery
- ‚úÖ **Reminder system** ‚Äî priority tones, nag behavior, ack tracking, Google Calendar 2-way sync
- ‚úÖ **Web Navigation Phase 2** ‚Äî result selection, page nav, scroll pagination, window management
- ‚úÖ **Developer tools skill** ‚Äî 13 intents, codebase search, git multi-repo, system admin, safety tiers
- ‚úÖ **Console mode** ‚Äî text/hybrid/speech modes with stats panel
- ‚úÖ **FIFINE K669B mic upgrade** ‚Äî udev rule, config updated

### Phase 8: Polish + Advanced Systems (Feb 15-17) üöÄ
- ‚úÖ **Kokoro TTS** ‚Äî 82M model, 50/50 fable+george blend, Piper fallback
- ‚úÖ **Latency refactor (4 phases)** ‚Äî streaming TTS, ack cache, streaming LLM, event pipeline
- ‚úÖ **User profile system (5 phases)** ‚Äî honorific, ProfileManager, SpeakerIdentifier, pipeline, enrollment
- ‚úÖ **Honorific refactoring** ‚Äî ~470 "sir" instances ‚Üí dynamic `{honorific}` across 19 files
- ‚úÖ **Conversational memory (6 phases)** ‚Äî SQLite facts, FAISS indexing, recall, batch extraction, proactive surfacing, forget/transparency
- ‚úÖ **Context window (4 phases)** ‚Äî topic-segmented working memory, relevance-scored assembly, cross-session persistence
- ‚úÖ **System health check** ‚Äî 5-layer diagnostic, ANSI terminal + voice summary
- ‚úÖ **Gapless TTS streaming** ‚Äî StreamingAudioPipeline, single persistent aplay, zero-gap playback
- ‚úÖ **Hardware failure graceful degradation** ‚Äî startup retry, device monitor, degraded mode

### Phase 9: Web Research + Hardening (Feb 17-18) üöÄ
- ‚úÖ **Web research (5 phases)** ‚Äî Qwen native tool calling + DuckDuckGo + trafilatura, multi-source synthesis
- ‚úÖ **Prescriptive prompt design** ‚Äî explicit rules for Qwen tool-use decisions, 150/150 correct test decisions
- ‚úÖ **Streaming delivery fixes** ‚Äî sentence-only chunking, per-chunk metric stripping, context flush on shutdown
- ‚úÖ **27 bug fixes** ‚Äî ack collision, keyword greediness, dismissal detection, decimal TTS, aplay lazy open, chunker decimal split, and more
- ‚úÖ **Scoped TTS subprocess control** ‚Äî replaced global `pkill -9` with tracked subprocess kill
- ‚úÖ **GitHub publishing system** ‚Äî automated redaction, PII verification, public repo sync

### Phase 10: Desktop Integration + Tooling (Feb 19-20) üöÄ
- ‚úÖ **GNOME Desktop Integration (5 phases)** ‚Äî Custom GNOME Shell extension with D-Bus bridge, 14 D-Bus methods
- ‚úÖ **Desktop Manager** ‚Äî Singleton module with lazy D-Bus, wmctrl fallback, pactl, notify-send, wl-clipboard
- ‚úÖ **App Launcher Skill v2.0** ‚Äî 16 intents: launch/close, fullscreen/minimize/maximize, volume, workspace, focus, clipboard
- ‚úÖ **Desktop notifications** ‚Äî Wired into reminder system via notify-send
- ‚úÖ **Publish script non-interactive mode** ‚Äî `--auto` flag for CI-friendly publish (auto-generate commit msg + push)

### Phase 11: Web Chat UI (Feb 20) üöÄ
- ‚úÖ **5-phase implementation** ‚Äî aiohttp WebSocket server, vanilla HTML/CSS/JS, zero new dependencies
- ‚úÖ **Streaming LLM** ‚Äî Token-by-token delivery with quality gate (buffers first sentence, retries if gibberish)
- ‚úÖ **File handling** ‚Äî Drag/drop, /file, /clipboard, /append, /context slash commands
- ‚úÖ **History + notifications** ‚Äî Paginated `/api/history`, scroll-to-load-more, floating announcement banners
- ‚úÖ **Polish** ‚Äî Markdown rendering with XSS protection, code blocks + copy, responsive breakpoints
- ‚úÖ **Session sidebar** ‚Äî 30-min gap detection, hamburger toggle, session rename, pagination, LIVE badge

### Phase 12: File Editor + Edge Case Testing (Feb 20) üöÄ
- ‚úÖ **File Editor Skill** ‚Äî 5 intents (write, edit, read, delete, list share), confirmation flow, LLM-generated content
- ‚úÖ **Ambient Wake Word Filter** ‚Äî Multi-signal: position, copula, threshold 0.80, length ‚Äî blocks ambient mentions
- ‚úÖ **Edge Case Testing Phase 1** ‚Äî ~200 test cases across 9 phases, 37/40 pass (92.5%), 14 routing failures fixed

### Phase 13: Conversational Flow Refactor (Feb 21) üöÄ
- ‚úÖ **Phase 1: Persona** ‚Äî 10 response pools (~50 templates), system prompts, honorific injection
- ‚úÖ **Phase 2: ConversationState** ‚Äî Turn counting, intent history, question detection, research context
- ‚úÖ **Phase 3: ConversationRouter** ‚Äî Shared priority chain for voice/console/web (one router, three frontends)
- ‚úÖ **Phase 4: Response Flow Polish** ‚Äî Contextual ack selection (10 tagged phrases), smarter follow-up windows, timeout cleanup, suppress LLM opener collision
- ‚úÖ **38 router tests** ‚Äî `scripts/test_router.py` validates routing decisions without live LLM/mic

### Phase 14: Whisper v2 Fine-Tuning (Feb 21) üöÄ
- ‚úÖ **198 training phrases** (up from 149), FIFINE K669B USB condenser mic
- ‚úÖ **GPU fp16 training** ‚Äî 89 seconds on RX 7900 XT
- ‚úÖ **94.4% live accuracy** ‚Äî wake word 100%, contraction handling 100%

### Phase 15: Document Generation + Demo Prep (Feb 22) üöÄ
- ‚úÖ **Document Generation** ‚Äî PPTX/DOCX/PDF via two-stage LLM pipeline, web research integration, Pexels images
- ‚úÖ **Smart Ack Suppression** ‚Äî Skip acknowledgments for fast/conversational queries
- ‚úÖ **Doc gen prompt overhaul** ‚Äî Prescriptive depth rules for Qwen
- ‚úÖ **7 live testing bugs fixed** ‚Äî publish.sh README protection
- ‚úÖ **Edge case tests expanded** ‚Äî 144 tests (Phase 1E)

### Phase 16: LLM Metrics Dashboard + Bug Fixes (Feb 23) üöÄ
- ‚úÖ **LLM Metrics Dashboard (5 phases)** ‚Äî Real-time tracking, web dashboard at `/metrics`, persistent SQLite, per-skill breakdowns
- ‚úÖ **jarvis-web.service** ‚Äî Systemd user service for web UI, auto-start after jarvis.service
- ‚úÖ **4 bug fixes** ‚Äî Web research timeout, desktop manager init order, health check PipeWire, audio PipeWire routing
- ‚úÖ **Preferred-mic hot-swap recovery** ‚Äî Device monitor recovers from wrong-mic fallback after boot race
- ‚úÖ **Edge case tests expanded** ‚Äî 152 tests (150/152 pass, 98.7%)

### Phase 17: Qwen3.5-35B-A3B Model Upgrade (Feb 24) üöÄ
- ‚úÖ **Qwen3.5-35B-A3B** ‚Äî MoE (256 experts, 8+1 active, ~3B active params), Q3_K_M quantization, 48-63 tok/s, IFEval 91.9
- ‚úÖ **llama.cpp rebuilt** ‚Äî ROCm build 8146, `GGML_HIP=ON`, 19.5/20.5 GB VRAM
- ‚úÖ **3 voice test fixes** ‚Äî Web search routing (removed stale keyword alias), ack bleed (0.35s settling delay), Whisper "quinn"‚Üí"qwen"
- ‚úÖ **Full doc overhaul** ‚Äî 12 files updated to reflect Qwen3.5-35B-A3B across all references
- ‚úÖ **Tier 4 LLM tests** ‚Äî 28 tests validating model response quality, tool calling, safety, hallucination resistance

### Phase 18: Self-Awareness + Task Planner (Feb 24-25) üöÄ
- ‚úÖ **Self-Awareness Layer** ‚Äî Capability manifest + system state injected into LLM context. JARVIS knows its own skills, latency, error rates, memory usage, and uptime
- ‚úÖ **Task Planner Phase 2** ‚Äî Compound request detection (22 conjunctive signals), LLM plan generation (JSON steps, max 4), sequential execution via skill_manager
- ‚úÖ **Task Planner Phase 3** ‚Äî Guardrails: destructive operation confirmation, failure-breaks, voice interrupts (cancel/skip via event queue)
- ‚úÖ **Task Planner Phase 4** ‚Äî Predictive timing announcements, error-aware planning (unreliable skill warnings), context-budget-aware planning (>80% warning), LLM per-step evaluation (continue/adjust/stop), pause/resume with 120s timeout
- ‚úÖ **5 bug fixes** ‚Äî Pause/resume guards (voice-only), eval timeout (10s), dead "skip that" phrase removed, 12 new tests

### Phase 19: Social Introductions + People Manager (Feb 25) üöÄ
- ‚úÖ **PeopleManager** ‚Äî `core/people_manager.py`: SQLite-backed people + facts database, TTS pronunciation overrides via normalizer, LLM context injection for known contacts
- ‚úÖ **Social Introductions Skill** ‚Äî 5 semantic intents (meet, who-is, recall, forget, update), multi-turn state machine (name confirm ‚Üí pronunciation check ‚Üí fact gathering ‚Üí complete)
- ‚úÖ **Persona expansion** ‚Äî 7 new response pools (~25 templates) for introduction flows
- ‚úÖ **Router P2.6** ‚Äî Introduction state machine intercept in ConversationRouter priority chain
- ‚úÖ **Edge case tests expanded** ‚Äî 270 tests (112 unit + 130 routing + 28 LLM), 100% pass rate

### Phase 20: LLM-Centric Tool Calling ‚Äî Phase 1 (Feb 26) üöÄ
- ‚úÖ **Native tool calling** ‚Äî 3 skills (time_info, system_info, filesystem) migrated from hard-coded routing to Qwen3.5 native tool calling
- ‚úÖ **Tool executor** ‚Äî `core/tool_executor.py`: Pure data dispatch, no TTS/skill dependencies
- ‚úÖ **P4-LLM routing** ‚Äî Semantic pruning selects relevant tools; LLM decides which to call. Non-migrated skill guard prevents over-capture
- ‚úÖ **All 3 frontends updated** ‚Äî Voice pipeline, console, web UI all handle tool-calling path
- ‚úÖ **100% accuracy** ‚Äî 600/600 trials across 60 queries √ó 10 runs, ~822ms average latency
- ‚úÖ **Test harness** ‚Äî `scripts/test_tool_calling.py`: 60 queries, 7-category taxonomy, `--sweep` for sampling matrix
- ‚úÖ **266/266 existing tests pass** ‚Äî Zero regressions from migration

---

## üé® Design Philosophy

### 1. Privacy First
All processing happens locally. No data leaves your machine. No telemetry, no cloud dependencies.

### 2. Modular & Extensible
Skills are independent modules. Add new capabilities without touching core code.

### 3. Natural Interaction
Semantic matching allows flexible phrasing. Say it naturally, JARVIS understands.

### 4. Production Quality
- Comprehensive error handling
- Extensive logging
- Graceful degradation
- Auto-recovery mechanisms

### 5. Hardware Efficient
Optimized for consumer hardware. No expensive GPUs required (though AMD GPU supported).

### 6. Maintainable
- Clean code structure
- Comprehensive documentation
- Version controlled
- Automated backups

---

## üó∫Ô∏è Roadmap

### Recently Completed
- Social introductions + People Manager (Feb 25)
- Self-awareness + task planner ‚Äî 4 phases (Feb 24-25)
- Qwen3.5-35B-A3B model upgrade ‚Äî MoE, Q3_K_M, 48-63 tok/s (Feb 24)
- LLM Metrics Dashboard ‚Äî 5 phases (Feb 23)
- Document Generation ‚Äî PPTX/DOCX/PDF (Feb 22)
- Whisper v2 fine-tuning ‚Äî 198 phrases, 94%+ accuracy (Feb 21)
- Conversational Flow Refactor ‚Äî 4 phases (Feb 21)
- Web Chat UI ‚Äî 5 phases (Feb 20)
- App launcher + desktop control ‚Äî 16 intents, GNOME Shell extension (Feb 19)
- Web research ‚Äî Qwen tool calling + DuckDuckGo (Feb 18)

### Up Next
- [ ] Inject user facts into web research
- [ ] "Onscreen please" ‚Äî retroactive visual display
- [ ] Document refinement follow-ups
- [ ] AI image generation (FLUX.1-schnell)
- [ ] Vision/OCR skill (Phase 1 Tesseract)

### Medium Term
- [ ] Audio recording skill
- [ ] LLM-centric architecture migration
- [ ] Email skill (Gmail)

### Long Term
- [ ] Threat hunting / malware analysis framework
- [ ] Video / face recognition
- [ ] Home automation
- [ ] Mobile access
- [ ] Emotional context awareness

---

## üöÄ Getting Started

### Prerequisites
- Ubuntu 24.04 LTS (or similar Linux)
- Python 3.11+
- 16GB+ RAM recommended
- GPU optional (AMD/NVIDIA for acceleration)

### Installation
```bash
# Clone repository
git clone <repo-url> ~/jarvis
cd ~/jarvis

# Install dependencies
pip install -r requirements.txt --break-system-packages

# Set up models directory
sudo mkdir -p /mnt/models
sudo chown $USER:$USER /mnt/models

# Download models (automated script coming soon)
# For now, manually place models in /mnt/models/

# Configure
cp config.yaml.example config.yaml
# Edit config.yaml with your settings

# Install services
cp jarvis.service ~/.config/systemd/user/
cp llama-server.service /etc/systemd/system/
systemctl --user daemon-reload
sudo systemctl daemon-reload

# Enable and start
systemctl --user enable --now jarvis
sudo systemctl enable --now llama-server

# Check status
systemctl --user status jarvis
```

### Quick Start
```bash
# Start JARVIS
startjarvis

# Stop JARVIS
stopjarvis

# Restart JARVIS
restartjarvis

# View logs
journalctl --user -u jarvis -f
```

### Basic Usage (Voice)
1. Say "Jarvis" to wake
2. Ask your question naturally
3. JARVIS responds
4. 4-7s adaptive window for follow-up (extends with conversation depth)

### Console Mode
```bash
python3 jarvis_console.py              # Text mode (type commands)
python3 jarvis_console.py --hybrid     # Text input + spoken output
```
Stats panel shows match layer, skill, confidence, timing, and LLM token counts after each command.

**Example Interactions:**
```
You: "Jarvis, what's the weather?"
JARVIS: "Currently 45 degrees and partly cloudy, sir."

You: "How about tomorrow?"
JARVIS: "Tomorrow's forecast shows 52 degrees with scattered showers, sir."

You: "How many lines of code in your codebase?"
JARVIS: "My codebase contains 320,388 lines of Python code across 40 files, sir."
```

---

## üìö Documentation

- **[SKILL_DEVELOPMENT.md](docs/SKILL_DEVELOPMENT.md)** - How to create skills
- **[DEVELOPMENT.md](docs/DEVELOPMENT.md)** - Development workflows
- **[CHANGELOG.md](CHANGELOG.md)** - Version history
- **[TODO_NEXT_SESSION.md](docs/TODO_NEXT_SESSION.md)** - Current priorities

---

## ü§ù Contributing

JARVIS is a personal project, but ideas and improvements are welcome!

### Adding Skills
1. Read [SKILL_DEVELOPMENT.md](docs/SKILL_DEVELOPMENT.md)
2. Create skill in `skills/` directory
3. Test thoroughly
4. Document in skill README

### Reporting Issues
Include:
- What you said
- What JARVIS responded
- Expected behavior
- Relevant logs

---

## üìä Performance Metrics

### Accuracy
- Wake word detection: 100% (Porcupine)
- Speech recognition: 94%+ (fine-tuned Whisper v2, 198 phrases, Southern accent)
- Intent matching: 95%+ (semantic embeddings)
- Routing tests: 38/38 pass (`scripts/test_router.py`)
- Edge case testing: 100% (236/236 across 106 unit + 130 routing tests + 28 LLM tests)

### Latency
- Wake word detection: <100ms
- Speech transcription: 0.1-0.2s (GPU-accelerated CTranslate2)
- Intent matching: <100ms (pre-computed semantic embedding cache)
- Skill-handled queries: 300-600ms total
- LLM fallback: 2-4s total (streaming)
- TTS generation: <1s (Kokoro streaming)

### Resource Usage
- RAM: ~4GB (with all models loaded)
- CPU: 10-30% during processing
- GPU: RX 7900 XT via ROCm (STT + LLM, 19.5/20.5 GB VRAM used)
- Disk: ~25GB (models + code)

---

## üéì What I've Learned

### Technical Insights
1. **Custom training beats generic models** - 94%+ vs 50% accuracy (198 phrases, 2 training rounds)
2. **REST APIs > subprocess calls** - More reliable for LLM
3. **Semantic matching is powerful** - Reduces pattern count 90%
4. **Preload heavy models** - Prevents audio thread blocking
5. **Log everything** - Makes debugging 10x easier
6. **One router, three frontends** - ConversationRouter eliminates routing duplication across voice/console/web
7. **Prescriptive > permissive for small LLMs** - Explicit numbered rules followed more reliably than prose instructions
8. **Substring `in` for keyword matching is a trap** - `"no" in "diagnostic"` is True. Always use word-boundary matching

### Development Practices
1. **Iterate quickly** - Small changes, frequent testing
2. **Test with real voice** - Keyboard input hides issues
3. **Monitor audio pipeline** - Overflow warnings are critical
4. **Version control everything** - Git saved me multiple times
5. **Document as you go** - Future you will thank you

### Design Decisions
1. **Offline first** - Privacy and reliability
2. **Modular skills** - Easy to extend and maintain
3. **Semantic intents** - Natural language flexibility
4. **British voice** - Character and professionalism
5. **Conservative responses** - Concise, helpful, polite

---

## üèÜ Achievements

- ‚úÖ Fully functional voice assistant with gapless streaming TTS
- ‚úÖ Custom accent training (fine-tuned Whisper v2, Southern accent, 94%+, 198 phrases)
- ‚úÖ Production-ready event-driven architecture
- ‚úÖ Web research via local LLM tool calling (no cloud required)
- ‚úÖ Conversational memory with semantic recall across sessions
- ‚úÖ Speaker identification and dynamic user profiles
- ‚úÖ 12 modular skills with semantic intent matching (including 16-intent desktop control + social introductions)
- ‚úÖ Self-awareness layer with capability manifest + system state for LLM context
- ‚úÖ Task planner with compound detection, LLM planning, per-step evaluation, pause/resume
- ‚úÖ Conversational flow engine with persona, state tracking, and shared router
- ‚úÖ Three frontends: voice, console, web UI (all sharing one router)
- ‚úÖ Web Chat UI with streaming, markdown, session sidebar, and health HUD
- ‚úÖ Ambient wake word filter (multi-signal, blocks false triggers)
- ‚úÖ 38 router tests + 270 edge case tests (100%) including 28 live LLM validation tests
- ‚úÖ Hardware failure graceful degradation
- ‚úÖ Sub-600ms skill responses (300-600ms)
- ‚úÖ Open source on GitHub with automated PII redaction

**JARVIS is a legitimate, production-ready AI assistant!**

---

## üìû Support

For issues or questions:
1. Check logs: `journalctl --user -u jarvis -n 100`
2. Review documentation
3. Test with simple commands first
4. Verify all services running

**Common Issues:**
- **No audio input:** Check microphone permissions
- **No TTS output:** Verify Kokoro/Piper installation
- **Intent not matching:** Lower threshold or add examples
- **LLM not responding:** Check llama-server status

---

**Built with ‚ù§Ô∏è and lots of coffee ‚òï**

*Built with care, tested obsessively, improved daily.*
